{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elOfoh-0RBcK"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncBNMMr2Rb1K"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "# from tensorflow.keras.applications import VGG16\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import os, shutil\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import models\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import seaborn\n",
        "\n",
        "print(keras.__version__)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5YIzLwTmBd5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset"
      ],
      "metadata": {
        "id": "0o7_CcNiK0UO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNikH5qZUTeF"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1KO0ryOH6j4pFYTJpSyGxL1iH1f84dNU7&export=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvu-ivtzpuQg"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1j9nLFoAA5xxC5DplQd-mgbysVsMcJEcz&export=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ3V7BMTpv1p"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1MWLGbv82_aEZo3h84pQAMRHkvCvsWSkA&export=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUaGaXz_rdFG"
      },
      "outputs": [],
      "source": [
        "!unzip /content/1.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2__hNCJG01_H"
      },
      "source": [
        "**Rename the zip files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCDMuNDmr9QX"
      },
      "outputs": [],
      "source": [
        "!unzip /content/0.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T8BxXCor_ZZ"
      },
      "outputs": [],
      "source": [
        "!unzip /content/2_3_4.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmAfxTOJtEQS"
      },
      "outputs": [],
      "source": [
        "!mkdir weather_detection\n",
        "%cd /content/weather_detection\n",
        "!mkdir train\n",
        "!mkdir test\n",
        "!mkdir validation\n",
        "%cd /content/weather_detection/test\n",
        "!mkdir Snowy\n",
        "!mkdir Cloudy\n",
        "!mkdir Sunny\n",
        "!mkdir Rainy\n",
        "!mkdir Foggy\n",
        "%cd /content/weather_detection/train\n",
        "!mkdir Snowy\n",
        "!mkdir Cloudy\n",
        "!mkdir Sunny\n",
        "!mkdir Rainy\n",
        "!mkdir Foggy\n",
        "%cd /content/weather_detection/validation\n",
        "!mkdir Snowy\n",
        "!mkdir Cloudy\n",
        "!mkdir Sunny\n",
        "!mkdir Rainy\n",
        "!mkdir Foggy\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viKjDK5xugkg"
      },
      "outputs": [],
      "source": [
        "print(len(os.listdir('/content/0')))\n",
        "print(len(os.listdir('/content/1')))\n",
        "print(len(os.listdir('/content/2')))\n",
        "print(len(os.listdir('/content/3')))\n",
        "print(len(os.listdir('/content/4')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting Dataset into Train Test Validate sets**"
      ],
      "metadata": {
        "id": "f5KRCSQ8K-sA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CboaFSUbtr_X"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,x_val=[],[],[]\n",
        "print('Train')\n",
        "l={'/content/0':1000,'/content/1':1000,'/content/2':800,'/content/3':700,'/content/4':300}\n",
        "k=['/content/weather_detection/train/Cloudy','/content/weather_detection/train/Sunny','/content/weather_detection/train/Rainy','/content/weather_detection/train/Snowy','/content/weather_detection/train/Foggy']\n",
        "for src,dst in zip(l.keys(),k):\n",
        "  i=1\n",
        "  for img in os.listdir(src):\n",
        "    shutil.move(src+f'/{img}',dst+f'/{img}')\n",
        "    img_arr=cv2.imread(dst+f'/{img}')\n",
        "    img_arr=cv2.resize(img_arr,(100,100))\n",
        "    x_train.append(img_arr)\n",
        "    if i==l[src]:\n",
        "      break\n",
        "    i+=1\n",
        "for i in k:\n",
        "  t=i.split('/')[-1]\n",
        "  print(t,':',len(os.listdir(i)))\n",
        "\n",
        "\n",
        "print('Validation')\n",
        "l={'/content/0':500,'/content/1':500,'/content/2':254,'/content/3':152,'/content/4':121}\n",
        "k=['/content/weather_detection/validation/Cloudy','/content/weather_detection/validation/Sunny','/content/weather_detection/validation/Rainy','/content/weather_detection/validation/Snowy','/content/weather_detection/validation/Foggy']\n",
        "for src,dst in zip(l.keys(),k):\n",
        "  i=1\n",
        "  for img in os.listdir(src):\n",
        "    shutil.move(src+f'/{img}',dst+f'/{img}')\n",
        "    \n",
        "    img_arr=cv2.imread(dst+f'/{img}')\n",
        "    img_arr=cv2.resize(img_arr,(100,100))\n",
        "    x_val.append(img_arr)\n",
        "\n",
        "    if i==l[src]:\n",
        "      break\n",
        "    i+=1\n",
        "for i in k:\n",
        "  t=i.split('/')[-1]\n",
        "  print(t,':',len(os.listdir(i)))\n",
        "\n",
        "\n",
        "print('Test')\n",
        "l={'/content/0':800,'/content/1':800,'/content/2':400,'/content/3':400,'/content/4':200}\n",
        "k=['/content/weather_detection/test/Cloudy','/content/weather_detection/test/Sunny','/content/weather_detection/test/Rainy','/content/weather_detection/test/Snowy','/content/weather_detection/test/Foggy']\n",
        "for src,dst in zip(l.keys(),k):\n",
        "  i=1\n",
        "  for img in os.listdir(src):\n",
        "    shutil.move(src+f'/{img}',dst+f'/{img}')\n",
        "    \n",
        "\n",
        "    img_arr=cv2.imread(dst+f'/{img}')\n",
        "    img_arr=cv2.resize(img_arr,(100,100))\n",
        "    x_test.append(img_arr)\n",
        "\n",
        "    if i==l[src]:\n",
        "      break\n",
        "    i+=1\n",
        "for i in k:\n",
        "  t=i.split('/')[-1]\n",
        "  print(t,':',len(os.listdir(i)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Images Array into Numpy Arrays"
      ],
      "metadata": {
        "id": "_8X4lYAiLjse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myjuvYSUqc2_"
      },
      "outputs": [],
      "source": [
        "train_x=np.array(x_train)\n",
        "test_x=np.array(x_test)\n",
        "val_x=np.array(x_val)\n",
        "\n",
        "########################################\n",
        "\n",
        "train_x=train_x/255.0\n",
        "test_x=test_x/255.0\n",
        "val_x=val_x/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "path to test,train and validation directories"
      ],
      "metadata": {
        "id": "PBpzxGpiLqX8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjLG43RKk8nV"
      },
      "outputs": [],
      "source": [
        "train_path='/content/weather_detection/train/'\n",
        "test_path='/content/weather_detection/test'\n",
        "val_path='/content/weather_detection/validation/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Data Generators"
      ],
      "metadata": {
        "id": "suv_j_oQL4m-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06I3UC3b-Bg2"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(train_path,\n",
        "                                                 target_size = (100, 100),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'sparse')\n",
        "test_set = test_datagen.flow_from_directory(test_path,\n",
        "                                            target_size = (100, 100),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'sparse')\n",
        "val_set = val_datagen.flow_from_directory(val_path,\n",
        "                                            target_size = (100, 100),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'sparse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lymW-l4Kl-Yk"
      },
      "outputs": [],
      "source": [
        "train_y=training_set.classes\n",
        "test_y=test_set.classes\n",
        "val_y=val_set.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puuK_YxcqE_D"
      },
      "outputs": [],
      "source": [
        "training_set.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFJf-9jbqHew"
      },
      "outputs": [],
      "source": [
        "train_y.shape,test_y.shape,val_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Model"
      ],
      "metadata": {
        "id": "XEGZ755FMC--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzZ3tMO3TYho"
      },
      "outputs": [],
      "source": [
        "model = VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False) # Shape of our images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8O17PrvolqC"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b6eDrhpTiXD"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Binary Classification"
      ],
      "metadata": {
        "id": "F1hDaU5BMckO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Flatten the output layer to 1 dimension\n",
        "# x = layers.Flatten()(model.output)\n",
        "\n",
        "# # Add a fully connected layer with 512 hidden units and ReLU activation\n",
        "# x = layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "# # Add a dropout rate of 0.5\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# # Add a final sigmoid layer with 1 node for classification output\n",
        "# x = layers.Dense(1, activation='sigmoid')(x)\n",
        "# model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])"
      ],
      "metadata": {
        "id": "NBW1Sxp2Mam8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Multi-class Classificatiion"
      ],
      "metadata": {
        "id": "LfWYhyxVMf6-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWwoEpm3TmIC"
      },
      "outputs": [],
      "source": [
        "x = Flatten()(model.output)\n",
        "prediction = Dense(5, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=model.input, outputs=prediction)\n",
        "\n",
        "model.compile(\n",
        "  loss='sparse_categorical_crossentropy',\n",
        "  optimizer=\"adam\",\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Early stopping"
      ],
      "metadata": {
        "id": "6gZLzI3qMkND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5NnixNnTthd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model using transfer learning"
      ],
      "metadata": {
        "id": "Fj-Ug4_CMpIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m87ZwWpzpyXO"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "  train_x,\n",
        "  train_y,\n",
        "  validation_data=(val_x,val_y),\n",
        "  epochs=100,\n",
        "  callbacks=[early_stop],\n",
        "  batch_size=32,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysing and Visualizing Training and validation results"
      ],
      "metadata": {
        "id": "TRdN3Dc4MvlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K6hJYCPxxL-"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'], label='train acc')\n",
        "\n",
        "plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig('vgg-acc-rps-1.png')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yJ7rtpX8x2oa"
      },
      "outputs": [],
      "source": [
        "# loss\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.savefig('vgg-loss-rps-1.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model"
      ],
      "metadata": {
        "id": "1d2vF6bPM7n_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZcY4hLFWcrZ"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/weather_detection/trained_model/vgg16_wheather_detection.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evalute the model on the test set"
      ],
      "metadata": {
        "id": "ONG67KQxM94s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8aCLFB_x-hD"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_x,test_y,batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysing the Testing results"
      ],
      "metadata": {
        "id": "CPNDMWklNDAx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgyNb4a-yJAs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "import numpy as np\n",
        "y_pred=model.predict(test_x)\n",
        "y_pred=np.argmax(y_pred,axis=1)\n",
        "#get classification report\n",
        "print(classification_report(y_pred,test_y))\n",
        "#get confusion matrix\n",
        "print(confusion_matrix(y_pred,test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_arFwFXjfan"
      },
      "source": [
        "**Analysing Failure test cases from saved models**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 1**"
      ],
      "metadata": {
        "id": "yiN6j7aTNuyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/weather_detection/Weather_detection_dataset.zip"
      ],
      "metadata": {
        "id": "0oxJaKtXNuM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset 2**"
      ],
      "metadata": {
        "id": "GpWXiUcGNxLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/weather_detection/dataset2new.zip"
      ],
      "metadata": {
        "id": "IDNTG3FLNmRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Saved Model**"
      ],
      "metadata": {
        "id": "C6CeirtuNz_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkTIfhyiEHkJ"
      },
      "outputs": [],
      "source": [
        "model = models.load_model('/content/drive/MyDrive/weather_detection/trained_model/2_DenseNet201_transf2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3owhLJhY3vWp"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model evalution"
      ],
      "metadata": {
        "id": "7duhxghGN8yL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxzUPUxrzV1I"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_x,test_y,batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seprating the Failure Test Cases"
      ],
      "metadata": {
        "id": "XE7C4OPdN_Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from keras.preprocessing import image"
      ],
      "metadata": {
        "id": "rZ5TYdgF2dID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing one image at a time"
      ],
      "metadata": {
        "id": "mSiMtZ96OTbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/alien_test/foggy_8.jpg'\n",
        "\n",
        "img = image.load_img(img_path,target_size=(224, 224, 3))\n",
        "# Convert it to a Numpy array with shape (150, 150, 3)\n",
        "test_x = image.img_to_array(img)\n",
        "# Reshape it to (1, 150, 150, 3)\n",
        "test_x = test_x.reshape((1,) + test_x.shape)\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "predict = model.predict(np.array(test_x))\n",
        "output = {0:'Cloudy',1:'Foggy',2:'Rainy',3:'Snowy',4:'Sunny'}\n",
        "plt.title(output[np.argmax(predict)])  \n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "rDx-gGsBfoXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing all images and seprate the failure test cases"
      ],
      "metadata": {
        "id": "pguWE_ZaOcff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8dLbXJdCyPp"
      },
      "outputs": [],
      "source": [
        "src='/content/Rainy'\n",
        "dst='/content/drive/MyDrive/weather_detection/Failed_DenseNet_dataset2/Rainy'\n",
        "k=0\n",
        "for name in os.listdir(src):\n",
        "  img_path = src+'/'+f'{name}'\n",
        "  img = image.load_img(img_path,target_size=(224, 224, 3))\n",
        "  # Convert it to a Numpy array with shape (150, 150, 3)\n",
        "  test_x = image.img_to_array(img)\n",
        "  # Reshape it to (1, 150, 150, 3)\n",
        "  test_x = test_x.reshape((1,) + test_x.shape)\n",
        "  test_x = test_x.astype('float32') / 255\n",
        "\n",
        "  predict = model.predict(np.array(test_x))\n",
        "  output = {0:'Cloudy',1:'Foggy',2:'Rainy',3:'Snowy',4:'Sunny'}\n",
        "  plt.title(output[np.argmax(predict)])  \n",
        "  plt.imshow(img)\n",
        "  if output[np.argmax(predict)]!='Rainy':\n",
        "    plt.savefig(dst+'/'+f'{name}')\n",
        "    k+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6mFpDKMUzwv"
      },
      "source": [
        "**Trying Different Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set All paths and define test generator"
      ],
      "metadata": {
        "id": "sRqH8GGYPj3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = '/content/Weather_detection_dataset/R_Train'\n",
        "testset = '/content/Weather_detection_dataset/R_Test'\n",
        "to_save = '/content/drive/MyDrive/weather_detection/trained_model'\n",
        "\n",
        "\"\"\"# Dataset split\"\"\"\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Generate batches of tensor image data with real-time data augmentation, here used to rescale and split the dataset\n",
        "#duplicates are removed by default\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale = 1. / 255,\n",
        "    rotation_range=30,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "#Takes the path to a directory & generates batches of augmented data.\n",
        "#When classes not provided, the list of classes is automatically inferred from the subdirectory names/structure\n",
        "#default batch = 32\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    directory = dataset,\n",
        "    target_size = (224, 224)\n",
        ")\n",
        "\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1. / 255,\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory = testset,\n",
        "    target_size = (224, 224)\n",
        ")\n",
        "\n",
        "classnames = [k for k,v in train_generator.class_indices.items()]\n",
        "print(\"Image input %s\" %str( train_generator.image_shape))\n",
        "print(\"Classes: %r \\n\" %classnames)\n",
        "\n",
        "print('Loaded %d training samples from %d classes.' %( train_generator.n, train_generator.num_classes))\n",
        "print('Loaded %d test samples from %d classes.' %( test_generator.n, test_generator.num_classes))"
      ],
      "metadata": {
        "id": "rSTGde1sG8Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show some examples"
      ],
      "metadata": {
        "id": "wBfsla-1PzRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x,y = train_generator.next()\n",
        "\n",
        "for i in range(0,10):\n",
        "    image = x[i]\n",
        "    label = y[i].argmax() \n",
        "    print(classnames[label])\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7zFvEnDnPxTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create different models"
      ],
      "metadata": {
        "id": "jwKH7PkNP_y2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2TclckPU9_v"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu', input_shape=train_generator.image_shape))\n",
        "model.add(layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'valid',  activation ='relu'))\n",
        "model.add(layers.AveragePooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
        "model.add(layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'valid', activation ='relu'))\n",
        "model.add(layers.AveragePooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation = \"relu\", ))\n",
        "model.add(layers.Dropout(0.3))\n",
        "model.add(layers.Dense(train_generator.num_classes, activation = \"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\"\"\"# Train the model\"\"\"\n",
        "\n",
        "import time\n",
        "\n",
        "#Total number of steps (batches of samples) to yield from generator before declaring one epoch finished\n",
        "steps = train_generator.n//train_generator.batch_size\n",
        "\n",
        "start = time.time()\n",
        "try:\n",
        "    history = model.fit_generator(train_generator, epochs=100, verbose=1,\n",
        "                    steps_per_epoch = steps,\n",
        "                    validation_data = test_generator,\n",
        "                    validation_steps = test_generator.n//test_generator.batch_size,\n",
        "                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=8, restore_best_weights=True)]\n",
        "                    )\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "end = time.time()\n",
        "enc_time = end-start\n",
        "\n",
        "print('Execution time:')\n",
        "print(str(enc_time))\n",
        "print()\n",
        "\n",
        "\"\"\"# Save the model\"\"\"\n",
        "\n",
        "filename = os.path.join(to_save, 'best1000.h5')\n",
        "model.save(filename)\n",
        "print(\"\\nModel saved successfully on file %s\\n\" %filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate The Model"
      ],
      "metadata": {
        "id": "AuDmYp-JQIIp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvxuljbeVqk2"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Evaluate the model\n",
        "\n",
        "Accuracy on test set\n",
        "\"\"\"\n",
        "\n",
        "#needed to put shuffle = false\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory = testset,\n",
        "    target_size = (224, 224),\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "val_steps = test_generator.n//test_generator.batch_size\n",
        "loss, acc = model.evaluate_generator( test_generator, verbose=1, steps = val_steps)\n",
        "print('Test loss: %f' %loss)\n",
        "print('Test accuracy: %f' %acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Precision, recall, F-score\"\"\"\n",
        "\n",
        "import sklearn.metrics \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('Loaded %d test samples from %d classes.' %(test_generator.n, test_generator.num_classes))\n",
        "\n",
        "preds = model.predict_generator(test_generator, verbose=1, steps=val_steps)\n",
        "\n",
        "Ypred = np.argmax(preds, axis=1)\n",
        "Ytest = test_generator.classes  # shuffle=False in test_generator\n"
      ],
      "metadata": {
        "id": "GINbH6ORp8fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(Ytest[:-3], Ypred, labels=None, target_names=classnames, digits=3))"
      ],
      "metadata": {
        "id": "xy6gKdR2u1-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"Graphs\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"confusion matrix\"\"\"\n",
        "\n",
        "cm = confusion_matrix(Ytest[:-3], Ypred)\n",
        "print(cm)\n",
        "\n",
        "conf = [] # data structure for confusions: list of (i,j,cm[i][j])\n",
        "for i in range(0,cm.shape[0]):\n",
        "  for j in range(0,cm.shape[1]):\n",
        "    if (i!=j and cm[i][j]>0):\n",
        "      conf.append([i,j,cm[i][j]])\n",
        "\n",
        "col=2\n",
        "conf = np.array(conf)\n",
        "conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])\n",
        "\n",
        "print('\\nConfusion matrix analysis' )\n",
        "print('%-16s     %-16s  \\t%s \\t%s ' %('True','Predicted','errors','err %'))\n",
        "print('------------------------------------------------------------------')\n",
        "for k in conf:\n",
        "  print('%-16s ->  %-16s  \\t%d \\t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/test_generator.n))"
      ],
      "metadata": {
        "id": "RIWXXvPNqABi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try VGG16 with some changes**"
      ],
      "metadata": {
        "id": "FzV8GBBuwKSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Create the transfer-model\"\"\"\n",
        "\n",
        "#feature extraction\n",
        "pretrained_model =VGG16(input_shape=train_generator.image_shape,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "IdLNGefxwHe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trying Xception**"
      ],
      "metadata": {
        "id": "7GhIoM7W5Ot2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import Xception\n",
        "pretrained_model =Xception(input_shape=train_generator.image_shape,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "jTpPguXJ5TQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trying with ResNet101**"
      ],
      "metadata": {
        "id": "xHqBWLD-FBf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet101V2\n",
        "pretrained_model =ResNet101V2(input_shape=train_generator.image_shape,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "bqRerFRjFF6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ResNet50V2**"
      ],
      "metadata": {
        "id": "B98g58HgK6CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50V2\n",
        "pretrained_model =ResNet50V2(input_shape=train_generator.image_shape,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LgkwvJ_bKoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DenseNet201**"
      ],
      "metadata": {
        "id": "PrRy8a0MQ0zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import DenseNet201\n",
        "pretrained_model =DenseNet201(input_shape=train_generator.image_shape,\n",
        "                                               include_top=False,weights=\"imagenet\")\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BrrgYYUpQ0cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VGG19**"
      ],
      "metadata": {
        "id": "kKJ14XkGbVar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "pretrained_model = VGG19(input_shape=train_generator.image_shape,\n",
        "                                               include_top=False,\n",
        "                                weights=\"imagenet\")\n",
        "pretrained_model.trainable = False\n",
        "model = tf.keras.Sequential([\n",
        "    pretrained_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NimHE3DvbULW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling and Fit the model"
      ],
      "metadata": {
        "id": "gU93KexsRX3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "import time\n",
        "\n",
        "#Total number of steps (batches of samples) to yield from generator before declaring one epoch finished\n",
        "steps = train_generator.n//train_generator.batch_size\n",
        "\n",
        "start = time.time()\n",
        "try:\n",
        "    history = model.fit_generator(train_generator, epochs=100, verbose=1,\n",
        "                    steps_per_epoch = steps,\n",
        "                    validation_data = test_generator,\n",
        "                    validation_steps = test_generator.n//test_generator.batch_size,\n",
        "                    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=8, restore_best_weights=True)]\n",
        "                    )\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "end = time.time()\n",
        "enc_time = end-start\n",
        "\n",
        "print('Execution time:')\n",
        "print(str(enc_time))\n",
        "print()\n",
        "\n",
        "\"\"\"# Save the model\"\"\"\n",
        "\n",
        "filename = os.path.join(to_save, 'VGG19_transf2.h5')\n",
        "model.save(filename)\n",
        "print(\"\\nModel saved successfully on file %s\\n\" %filename)"
      ],
      "metadata": {
        "id": "5yk2Mfv3xBv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evalute the models**"
      ],
      "metadata": {
        "id": "EVVCsd1ZdQuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Evaluate the model\n",
        "\n",
        "Accuracy on test set\n",
        "\"\"\"\n",
        "\n",
        "#needed to put shuffle = false\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory = testset,\n",
        "    target_size = (224, 224),\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "val_steps = test_generator.n//test_generator.batch_size\n",
        "loss, acc = model.evaluate_generator( test_generator, verbose=1, steps = val_steps)\n",
        "print('Test loss: %f' %loss)\n",
        "print('Test accuracy: %f' %acc)"
      ],
      "metadata": {
        "id": "la3TqjIYxLWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Precision, recall, F-score\"\"\"\n",
        "\n",
        "import sklearn.metrics \n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('Loaded %d test samples from %d classes.' %(test_generator.n, test_generator.num_classes))\n",
        "\n",
        "preds = model.predict_generator(test_generator, verbose=1, steps=val_steps)\n",
        "\n",
        "Ypred = np.argmax(preds, axis=1)\n",
        "Ytest = test_generator.classes  # shuffle=False in test_generator\n",
        "\n",
        "print(classification_report(Ytest[:-3], Ypred, labels=None, target_names=classnames, digits=3))"
      ],
      "metadata": {
        "id": "bc99f7-z30Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"Graphs\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"confusion matrix\"\"\"\n",
        "\n",
        "cm = confusion_matrix(Ytest[:-3], Ypred)\n",
        "print(cm)\n",
        "\n",
        "conf = [] # data structure for confusions: list of (i,j,cm[i][j])\n",
        "for i in range(0,cm.shape[0]):\n",
        "  for j in range(0,cm.shape[1]):\n",
        "    if (i!=j and cm[i][j]>0):\n",
        "      conf.append([i,j,cm[i][j]])\n",
        "\n",
        "col=2\n",
        "conf = np.array(conf)\n",
        "conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])\n",
        "\n",
        "print('\\nConfusion matrix analysis' )\n",
        "print('%-16s     %-16s  \\t%s \\t%s ' %('True','Predicted','errors','err %'))\n",
        "print('------------------------------------------------------------------')\n",
        "for k in conf:\n",
        "  print('%-16s ->  %-16s  \\t%d \\t%.2f %% ' %(classnames[k[0]],classnames[k[1]],k[2],k[2]*100.0/test_generator.n))\n"
      ],
      "metadata": {
        "id": "xEI1jKJt38i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Blind Test**"
      ],
      "metadata": {
        "id": "9UQjxYFIR5AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"# blind test\"\"\"\n",
        "\n",
        "# files = '/content/drive/My Drive/WeatherBlindTestSet'\n",
        "\n",
        "# test_generator = test_datagen.flow_from_directory(\n",
        "#     directory = files,\n",
        "#     target_size = (224, 224),\n",
        "#     shuffle=False\n",
        "# )\n",
        "\n",
        "\n",
        "# val_steps = test_generator.n//test_generator.batch_size+1\n",
        "# preds = model.predict_generator(test_generator, verbose=1, steps=val_steps)\n",
        "\n",
        "# print(preds)\n",
        "\n",
        "# Ypred = np.argmax(preds, axis=1)\n",
        "# np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "\n",
        "# res = []\n",
        "# sun = 0\n",
        "# haz = 0 \n",
        "# ran = 0 \n",
        "# sn = 0;\n",
        "# for i in Ypred:\n",
        "#   if i == 0:\n",
        "#     res.append('HAZE')\n",
        "#     haz += 1\n",
        "#   if i == 1:\n",
        "#     res.append('RAINY')\n",
        "#     ran += 1\n",
        "#   if i == 2:\n",
        "#     res.append('SNOWY')\n",
        "#     sn += 1\n",
        "#   if i == 3:\n",
        "#     res.append('SUNNY')\n",
        "#     sun += 1\n",
        "\n",
        "# print(res)\n",
        "# print(Ypred)\n",
        "\n",
        "# print(len(res))\n",
        "# print(len(Ypred))\n",
        "# print(sun)\n",
        "# print(haz)\n",
        "# print(ran)\n",
        "# print(sn)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# results = pd.DataFrame(res, columns=['predictions'])\n",
        "# from google.colab import files\n",
        "# results.to_csv('prediction.csv') \n",
        "# files.download('prediction.csv')"
      ],
      "metadata": {
        "id": "VaNbpYkXR62u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Weather_detection.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}